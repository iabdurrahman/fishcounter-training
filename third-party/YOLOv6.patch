diff --git a/README.md b/README.md
index 4aa94ab..f483e41 100644
--- a/README.md
+++ b/README.md
@@ -1,3 +1,15 @@
+#### Get model optimized for RKNN
+Exports model with optimization for RKNN, please refer here [RKOPT_README.md](./deploy/RKNN/RKOPT_README.md)
+
+
+This optimization only affects the export of the model and does not affect the training process. **For the training steps, please refer to the YOLOv6 official documentation.**
+
+
+
+
+---
+
+
 <p align="center">
   <img src="assets/banner-YOLO.png" align="middle" width = "1000" />
 </p>
@@ -30,6 +42,11 @@ Implementation of paper:
 - [2023.03.10] Release [YOLOv6-Face](https://github.com/meituan/YOLOv6/tree/yolov6-face). ğŸ”¥ [Performance](https://github.com/meituan/YOLOv6/tree/yolov6-face#performance-on-widerface)
 - [2023.03.02] Update [base models](configs/base/README.md) to version 3.0.
 - [2023.01.06] Release P6 models and enhance the performance of P5 models. â­ï¸ [Benchmark](#Benchmark)
+    - Renew the neck of the detector with a BiC module and SimCSPSPPF Block.
+    - Propose an anchor-aided training (AAT) strategy.
+    - Involve a new self-distillation strategy for small models of YOLOv6.
+    - Expand YOLOv6 and hit a new
+    SOTA performance on the COCO dataset.
 - [2022.11.04] Release [base models](configs/base/README.md) to simplify the training and deployment process.
 - [2022.09.06] Customized quantization methods. ğŸš€ [Quantization Tutorial](./tools/qat/README.md)
 - [2022.09.05] Release M/L models and update N/T/S models with enhanced performance.
diff --git a/README_cn.md b/README_cn.md
index d7181bc..ea2da4a 100644
--- a/README_cn.md
+++ b/README_cn.md
@@ -1,3 +1,13 @@
+#### å¯¼å‡ºé€‚é… rknpu çš„æ¨¡å‹
+é€‚é… rknpu çš„æ¨¡å‹ç»“æ„å¯ä»¥åœ¨ npu ä¸Šè·å¾—æ›´é«˜çš„æ¨ç†æ•ˆç‡ã€‚å…³äºå¯¼å‡ºç»†èŠ‚è¯·å‚è€ƒ  [RKOPT_README_zh.md](./deploy/RKNN/RKOPT_README_cn.md)
+
+æ­¤ä¼˜åŒ–åªå½±å“äº†æ¨¡å‹çš„å¯¼å‡ºï¼Œä¸å½±å“è®­ç»ƒè¿‡ç¨‹ï¼Œ**è®­ç»ƒæ­¥éª¤è¯·å‚è€ƒ YOLOv6 å®˜æ–¹æ–‡æ¡£**ã€‚
+
+
+
+
+---
+
 <p align="center">
   <img src="assets/banner-YOLO.png" align="middle" width = "1000" />
 </p>
@@ -20,6 +30,10 @@
 - [2023.03.10] å‘å¸ƒ [YOLOv6-Face](https://github.com/meituan/YOLOv6/tree/yolov6-face). ğŸ”¥ [äººè„¸æ£€æµ‹æ¨¡å‹æŒ‡æ ‡](https://github.com/meituan/YOLOv6/blob/yolov6-face/README_cn.md#widerface-%E6%A8%A1%E5%9E%8B%E6%8C%87%E6%A0%87)
 - [2023.03.02] æ›´æ–° [åŸºç¡€ç‰ˆæ¨¡å‹](configs/base/README_cn.md) åˆ° 3.0 ç‰ˆæœ¬
 - [2023.01.06] å‘å¸ƒå¤§åˆ†è¾¨ç‡ P6 æ¨¡å‹ä»¥åŠå¯¹ P5 æ¨¡å‹åšäº†å…¨é¢çš„å‡çº§ â­ï¸ [æ¨¡å‹æŒ‡æ ‡](#æ¨¡å‹æŒ‡æ ‡)
+    - æ·»åŠ  BiC æ¨¡å— å’Œ SimCSPSPPF æ¨¡å—ä»¥å¢å¼ºæ£€æµ‹ç½‘ç»œé¢ˆéƒ¨çš„è¡¨å¾èƒ½åŠ›ã€‚
+    - æå‡ºä¸€ä¸ªé”šç‚¹è¾…åŠ©è®­ç»ƒ (AAT) ç­–ç•¥ã€‚
+    - ä¸º YOLOv6 å°æ¨¡å‹å¼•å…¥ä¸€ä¸ªæ–°çš„è‡ªè’¸é¦è®­ç»ƒç­–ç•¥ã€‚
+    - æ‰©å±• YOLOv6 å¹¶åœ¨ COCO ä¸Šå–å¾—äº†å®æ—¶ç›®æ ‡æ£€æµ‹ SOTA çš„ç²¾åº¦å’Œé€Ÿåº¦ã€‚
 - [2022.11.04] å‘å¸ƒ [åŸºç¡€ç‰ˆæ¨¡å‹](configs/base/README_cn.md) ç®€åŒ–è®­ç»ƒéƒ¨ç½²æµç¨‹
 - [2022.09.06] å®šåˆ¶åŒ–çš„æ¨¡å‹é‡åŒ–åŠ é€Ÿæ–¹æ³• ğŸš€ [é‡åŒ–æ•™ç¨‹](./tools/qat/README.md)
 - [2022.09.05] å‘å¸ƒ M/L æ¨¡å‹ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥æé«˜äº† N/T/S æ¨¡å‹çš„æ€§èƒ½
diff --git a/deploy/RKNN/RKOPT_README.md b/deploy/RKNN/RKOPT_README.md
new file mode 100644
index 0000000..0524a02
--- /dev/null
+++ b/deploy/RKNN/RKOPT_README.md
@@ -0,0 +1,50 @@
+## Description - export optimized model for RKNPU
+
+### 1. Model structure Adjustment
+
+- The dfl structure has poor performance on NPU processing, moved outside the model.
+
+  Assuming that there are 6000 candidate frames, the original model places the dfl structure before the "box confidence filter", then the 6000 candidate frames need to be calculated through dfl calculation. If the dfl structure is placed after the "box confidence filter", Assuming that there are 100 candidate boxes left after filtering, the calculation amount of the dfl part is reduced to 100, which greatly reduces the occupancy of computing resources and bandwidth resources.
+
+- Notice:  yolov6n/s  hasn't  dfl structure, while yolov6m/l has dfl structure
+
+
+
+- Assuming that there are 6000 candidate boxes and the detection category is 80, the threshold retrieval operation needs to be repeated 6000* 80 ~= 4.8*10^5 times, which takes a lot of time. Therefore, when exporting the model, an additional summation operation for 80 types of detection targets is added to the model to quickly filter the confidence. (This structure is effective in some cases, related to the training results of the model)
+
+  (v6m, v6l) To disable this optimization,  comment the following code in ./yolov6/models/effidehead.py (line70~86 part)
+
+  ```
+  cls_sum = torch.clamp(y[-1].sum(1, keepdim=True), 0, 1)
+  output_for_rknn.append(cls_sum)
+  ```
+
+  (v6n, v6s) To disable this optimization,  comment the following code in  ./yolov6/models/heads/effidehead_distill_ns.py (line78~94 part)
+  
+  ```
+  cls_sum = torch.clamp(y[-1].sum(1, keepdim=True), 0, 1)
+  output_for_rknn.append(cls_sum)
+  ```
+  
+
+
+
+- This optimization only affects the export of the model and does not affect the training process. **For the training steps, please refer to the YOLOv6 official documentation.**
+
+
+
+### 2. Export model operation
+
+After meeting the environmental requirements of ./requirements.txt, execute the following statement to export the model
+
+```
+python deploy/RKNN/export_onnx_for_rknn.py --weight ./yolov6n.pt
+
+# adjust ./yolov6n.pt path to export your model.
+```
+
+
+
+### 3.Transfer to RKNN model, Python demo, C demo
+
+Please refer https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo 
\ No newline at end of file
diff --git a/deploy/RKNN/RKOPT_README_cn.md b/deploy/RKNN/RKOPT_README_cn.md
new file mode 100644
index 0000000..904eb26
--- /dev/null
+++ b/deploy/RKNN/RKOPT_README_cn.md
@@ -0,0 +1,51 @@
+## RKNN å¯¼å‡ºæ¨¡å‹è¯´æ˜
+
+### 1.è°ƒæ•´éƒ¨åˆ†
+
+- ç”±äº dfl ç»“æ„åœ¨ npu å¤„ç†æ€§èƒ½ä¸ä½³ã€‚å‡è®¾æœ‰6000ä¸ªå€™é€‰æ¡†ï¼ŒåŸæ¨¡å‹å°† dfl ç»“æ„æ”¾ç½®äº ''æ¡†ç½®ä¿¡åº¦è¿‡æ»¤" å‰ï¼Œåˆ™ 6000 ä¸ªå€™é€‰æ¡†éƒ½éœ€è¦è®¡ç®—ç»è¿‡ dfl è®¡ç®—ï¼›è€Œå°† dfl ç»“æ„æ”¾ç½®äº ''æ¡†ç½®ä¿¡åº¦è¿‡æ»¤" åï¼Œå‡è®¾è¿‡ç¨‹æˆ 100 ä¸ªå€™é€‰æ¡†ï¼Œåˆ™dfléƒ¨åˆ†è®¡ç®—é‡å‡å°‘è‡³ 100 ä¸ªã€‚
+
+  æ•…å°† dfl ç»“æ„ä½¿ç”¨ cpu å¤„ç†çš„è€—æ—¶ï¼Œè™½ç„¶äº«å—ä¸åˆ° npu åŠ é€Ÿï¼Œä½†æ˜¯æœ¬æ¥å¸¦æ¥çš„è®¡ç®—é‡è¾ƒå°‘ä¹Ÿæ˜¯å¾ˆå¯è§‚çš„ã€‚
+  
+  æ³¨ï¼š yolov6n, yolov6s æ²¡æœ‰ dfl ç»“æ„; yolov6m, yolov6l å­˜åœ¨ dfl ç»“æ„
+
+
+
+- å‡è®¾å­˜åœ¨ 6000 ä¸ªå€™é€‰æ¡†ï¼Œå­˜åœ¨ 80 ç±»æ£€æµ‹ç›®æ ‡ï¼Œåˆ™é˜ˆå€¼éœ€è¦æ£€ç´¢çš„ç½®ä¿¡åº¦æœ‰ 6000* 80 ï½= 4.8*10^5 ä¸ªï¼Œå æ®äº†è¾ƒå¤šè€—æ—¶ï¼Œæ•…å¯¼å‡ºæ¨¡å‹æ—¶ï¼Œåœ¨æ¨¡å‹ä¸­é¢å¤–æ–°å¢äº†å¯¹ 80 ç±»æ£€æµ‹ç›®æ ‡è¿›è¡Œæ±‚å’Œæ“ä½œï¼Œç”¨äºå¿«é€Ÿè¿‡æ»¤ç½®ä¿¡åº¦ï¼Œè¯¥ç»“æ„åœ¨éƒ¨åˆ†æƒ…å†µä¸‹å¯¹æ¨¡å‹æœ‰æ•ˆã€‚
+
+  (v6m, v6l) å¯ä»¥åœ¨ ./yolov6/models/effidehead.py 70~86è¡Œä½ç½®ï¼Œæ³¨é‡Šæ‰è¿™éƒ¨åˆ†
+
+  ```
+  cls_sum = torch.clamp(y[-1].sum(1, keepdim=True), 0, 1)
+  output_for_rknn.append(cls_sum)
+  ```
+
+  (v6n, v6s) å¯ä»¥åœ¨  yolov6/models/heads/effidehead_distill_ns.py 78~94è¡Œä½ç½®ï¼Œæ³¨é‡Šæ‰è¿™éƒ¨åˆ†
+  
+  ```
+  cls_sum = torch.clamp(y[-1].sum(1, keepdim=True), 0, 1)
+  output_for_rknn.append(cls_sum)
+  ```
+  
+
+
+
+- ä»¥ä¸Šä¼˜åŒ–åªå½±å“äº†æ¨¡å‹çš„å¯¼å‡ºï¼Œä¸å½±å“è®­ç»ƒè¿‡ç¨‹ï¼Œ**è®­ç»ƒæ­¥éª¤è¯·å‚è€ƒ YOLOv6 å®˜æ–¹æ–‡æ¡£**ã€‚
+
+
+
+
+### 2.å¯¼å‡ºæ¨¡å‹æ“ä½œ
+
+åœ¨æ»¡è¶³ ./requirements.txt çš„ç¯å¢ƒè¦æ±‚åï¼Œæ‰§è¡Œä»¥ä¸‹è¯­å¥å¯¼å‡ºæ¨¡å‹
+
+```
+python deploy/RKNN/export_onnx_for_rknn.py --weight ./yolov6n.pt
+
+# å¦‚æœè‡ªå·±è®­ç»ƒæ¨¡å‹ï¼Œåˆ™è·¯å¾„./yolov6n.pt è¯·æ”¹ä¸ºè‡ªå·±æ¨¡å‹çš„è·¯å¾„
+```
+
+
+
+### 3.è½¬RKNNæ¨¡å‹ã€Python demoã€C demo
+
+è¯·å‚è€ƒ https://github.com/airockchip/rknn_model_zoo/tree/main/models/CV/object_detection/yolo 
\ No newline at end of file
diff --git a/deploy/RKNN/export_onnx_for_rknn.py b/deploy/RKNN/export_onnx_for_rknn.py
new file mode 100755
index 0000000..e6f4d0e
--- /dev/null
+++ b/deploy/RKNN/export_onnx_for_rknn.py
@@ -0,0 +1,80 @@
+#!/usr/bin/env python3
+# -*- coding:utf-8 -*-
+import argparse
+import time
+import sys
+import os
+import torch
+import torch.nn as nn
+import onnx
+
+ROOT = os.getcwd()
+if str(ROOT) not in sys.path:
+    sys.path.append(str(ROOT))
+
+from yolov6.models.yolo import *
+from yolov6.models.effidehead import Detect
+from yolov6.layers.common import *
+from yolov6.utils.events import LOGGER
+from yolov6.utils.checkpoint import load_checkpoint
+
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser()
+    parser.add_argument('--weights', type=str, default='./yolov6s6.pt', help='weights path')
+    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='image size')  # height, width
+    parser.add_argument('--batch-size', type=int, default=1, help='batch size')
+    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')
+    parser.add_argument('--inplace', action='store_true', help='set Detect() inplace=True')
+    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
+    args = parser.parse_args()
+    args.img_size *= 2 if len(args.img_size) == 1 else 1  # expand
+    print(args)
+    t = time.time()
+
+    # Check device
+    cuda = args.device != 'cpu' and torch.cuda.is_available()
+    device = torch.device('cuda:0' if cuda else 'cpu')
+    assert not (device.type == 'cpu' and args.half), '--half only compatible with GPU export, i.e. use --device 0'
+    # Load PyTorch model
+    model = load_checkpoint(args.weights, map_location=device, inplace=True, fuse=True)  # load FP32 model
+    for layer in model.modules():
+        if isinstance(layer, RepVGGBlock):
+            layer.switch_to_deploy()
+        elif isinstance(layer, nn.Upsample) and not hasattr(layer, 'recompute_scale_factor'):
+            layer.recompute_scale_factor = None  # torch 1.11.0 compatibility
+
+    # Input
+    img = torch.zeros(args.batch_size, 3, *args.img_size).to(device)  # image size(1,3,320,192) iDetection
+
+    # Update model
+    if args.half:
+        img, model = img.half(), model.half()  # to FP16
+    model.eval()
+    for k, m in model.named_modules():
+        if isinstance(m, ConvModule):  # assign export-friendly activations
+            if hasattr(m, 'act') and isinstance(m.act, nn.SiLU):
+                m.act = SiLU()
+        elif isinstance(m, Detect):
+            m.inplace = args.inplace
+
+    model.detect.export_rknn = True
+    y = model(img)  # dry run
+
+    # ONNX export
+    try:
+        LOGGER.info('\nStarting to export ONNX with rknn-optimized...')
+        export_file = args.weights.replace('.pt', '.onnx')  # filename
+        torch.onnx.export(model, img, export_file, verbose=False, opset_version=12,
+                          training=torch.onnx.TrainingMode.EVAL,
+                          do_constant_folding=True,
+                          input_names=['image_arrays'],
+                          output_names=['outputs'],
+                         )
+
+        # Checks
+        onnx_model = onnx.load(export_file)  # load onnx model
+        onnx.checker.check_model(onnx_model)  # check onnx model
+        LOGGER.info(f'ONNX with rknn-optimized export success, saved as {export_file}')
+    except Exception as e:
+        LOGGER.info(f'ONNX export failure: {e}')
\ No newline at end of file
diff --git a/tools/test_load.py b/tools/test_load.py
new file mode 100755
index 0000000..eece025
--- /dev/null
+++ b/tools/test_load.py
@@ -0,0 +1,62 @@
+#!/usr/bin/env python
+import pathlib
+import os
+import sys
+
+import torch
+
+from torch.serialization import add_safe_globals
+
+import numpy
+
+from torch.nn.modules.activation import ReLU, SiLU
+from torch.nn.modules.linear     import Identity
+from torch.nn.modules.conv       import Conv2d, ConvTranspose2d
+from torch.nn.modules.batchnorm  import BatchNorm2d
+from torch.nn.modules.container  import Sequential, ModuleList
+from torch.nn.modules.pooling    import MaxPool2d
+
+ROOT = os.getcwd()
+if str(ROOT) not in sys.path:
+    sys.path.append(str(ROOT))
+
+#from yolov6.models.efficientrep  import EfficientRep, EfficientRep6, CSPBepBackbone, CSPBepBackbone_P6, Lite_EffiBackbone
+from yolov6.models.efficientrep  import EfficientRep
+from yolov6.models.effidehead    import Detect
+#from yolov6.models.end2end       import ORT_NMS, TRT8_NMS, TRT7_NMS, ONNX_ORT, ONNX_TRT7, ONNX_TRT8, End2End
+#from yolov6.models.reppan        import RepPANNeck, RepBiFPANNeck, RepPANNeck6, RepBiFPANNeck6, CSPRepPANNeck, CSPRepBiFPANNeck, CSPRepPANNeck_P6, CSPRepBiFPANNeck_P6, Lite_EffiNeck
+from yolov6.models.reppan        import RepBiFPANNeck
+from yolov6.models.yolo          import Model as YOLOv6_Model
+#from yolov6.models.yolo_lite     import Model as YOLOv6_Model_Lite
+#from yolov6.layers.common        import *
+from yolov6.layers.common        import RepVGGBlock, ConvModule, RepBlock, SimCSPSPPF, CSPSPPFModule, ConvBNReLU, BiFusion, Transpose, ConvBNSiLU
+
+ckpt_dir = pathlib.Path("runs/train/exp/weights")
+
+#add_safe_globals([EfficientRep, EfficientRep6, CSPBepBackbone, CSPBepBackbone_P6, Lite_EffiBackbone,
+#	Detect,
+#	ORT_NMS, TRT8_NMS, TRT7_NMS, ONNX_ORT, ONNX_TRT7, ONNX_TRT8, End2End,
+#	RepPANNeck, RepBiFPANNeck, RepPANNeck6, RepBiFPANNeck6, CSPRepPANNeck, CSPRepBiFPANNeck, CSPRepPANNeck_P6, CSPRepBiFPANNeck_P6, Lite_EffiNeck,
+#	(YOLOv6_Model_Lite,  "yolov6.models.yolo_lite.Model"),
+#	SiLU, ConvModule, ConvBNReLU, ConvBNSiLU, ConvBN, ConvBNHS, SPPFModule, SimSPPF, SPPF, CSPSPPFModule, SimCSPSPPF, CSPSPPF, Transpose,
+#	RepVGGBlock, QARepVGGBlock, QARepVGGBlockV2, RealVGGBlock, ScaleLayer, LinearAddBlock, DetectBackend, RepBlock, BottleRep, BottleRep3,
+#	BepC3, MBLABlock, BiFusion, SEBlock, Lite_EffiBlockS1, Lite_EffiBlockS2, DPBlock, DarknetBlock, CSPBlock,
+add_safe_globals([
+	EfficientRep,
+	Detect,
+	RepBiFPANNeck,
+	(YOLOv6_Model,       "yolov6.models.yolo.Model"),
+	RepVGGBlock, ConvModule, RepBlock, SimCSPSPPF, CSPSPPFModule, ConvBNReLU, BiFusion, Transpose, ConvBNSiLU,
+	numpy._core.multiarray.scalar, numpy.dtype, numpy.dtypes.Float64DType,
+	ReLU, SiLU,
+	Identity,
+	Conv2d, ConvTranspose2d,
+	BatchNorm2d,
+	Sequential, ModuleList,
+	MaxPool2d])
+
+for s in ["best", "last"]:
+	ckpt_path = ckpt_dir / ("%s_ckpt.pt" % (s, ))
+	if (not ckpt_path.exists()):
+		raise FileNotFoundError()
+	ckpt = torch.load(ckpt_path, map_location=torch.device("cpu"), weights_only=True)
diff --git a/yolov6/core/engine.py b/yolov6/core/engine.py
index 2de165b..b75b514 100644
--- a/yolov6/core/engine.py
+++ b/yolov6/core/engine.py
@@ -43,7 +43,7 @@ class Trainer:
         self.max_epoch = args.epochs
 
         if args.resume:
-            self.ckpt = torch.load(args.resume, map_location='cpu')
+            self.ckpt = torch.load(args.resume, map_location='cpu', weights_only=False)
 
         self.rank = args.rank
         self.local_rank = args.local_rank
@@ -447,7 +447,7 @@ class Trainer:
         if not weights:
             LOGGER.error("ERROR: No scales provided to init RepOptimizer!")
         else:
-            ckpt = torch.load(weights, map_location=device)
+            ckpt = torch.load(weights, map_location=device, weights_only=False)
             scales = extract_scales(ckpt)
         return scales
 
diff --git a/yolov6/data/datasets.py b/yolov6/data/datasets.py
index e00bd05..daba5fc 100644
--- a/yolov6/data/datasets.py
+++ b/yolov6/data/datasets.py
@@ -599,7 +599,20 @@ class TrainValDataset(Dataset):
     @staticmethod
     def generate_coco_format_labels(img_info, class_names, save_path):
         # for evaluation with pycocotools
-        dataset = {"categories": [], "annotations": [], "images": []}
+        dataset = {
+            "info": {
+                "description": "Converted from YOLO format",
+                "version": "1.0",
+                "year": 2025,
+                "contributor": "",
+                "date_created": ""
+            },
+            "licenses": [],
+            "categories": [],
+            "annotations": [],
+            "images": []
+        }
+
         for i, class_name in enumerate(class_names):
             dataset["categories"].append(
                 {"id": i, "name": class_name, "supercategory": ""}
diff --git a/yolov6/models/effidehead.py b/yolov6/models/effidehead.py
index 55b7b06..0de2ee5 100644
--- a/yolov6/models/effidehead.py
+++ b/yolov6/models/effidehead.py
@@ -68,7 +68,28 @@ class Detect(nn.Module):
         self.proj_conv.weight = nn.Parameter(self.proj.view([1, self.reg_max + 1, 1, 1]).clone().detach(),
                                                    requires_grad=False)
 
+    def _rknn_opt_head(self, x):
+        output_for_rknn = []
+        for i in range(self.nl):
+            x[i] = self.stems[i](x[i])
+            reg_feat = self.reg_convs[i](x[i])
+            reg_output = self.reg_preds[i](reg_feat)
+
+            cls_feat = self.cls_convs[i](x[i])
+            cls_output = self.cls_preds[i](cls_feat)
+            cls_output = torch.sigmoid(cls_output)
+
+            cls_sum = torch.clamp(cls_output.sum(1, keepdim=True), 0, 1)
+
+            output_for_rknn.append(reg_output)
+            output_for_rknn.append(cls_output)
+            output_for_rknn.append(cls_sum)
+        return output_for_rknn
+
     def forward(self, x):
+        if getattr(self, "export_rknn", False):
+            return self._rknn_opt_head(x)
+
         if self.training:
             cls_score_list = []
             reg_distri_list = []
diff --git a/yolov6/models/heads/effidehead_distill_ns.py b/yolov6/models/heads/effidehead_distill_ns.py
index 912bd6c..66e87e5 100644
--- a/yolov6/models/heads/effidehead_distill_ns.py
+++ b/yolov6/models/heads/effidehead_distill_ns.py
@@ -76,7 +76,28 @@ class Detect(nn.Module):
         self.proj_conv.weight = nn.Parameter(self.proj.view([1, self.reg_max + 1, 1, 1]).clone().detach(),
                                                    requires_grad=False)
 
+    def _rknn_opt_head(self, x):
+        output_for_rknn = []
+        for i in range(self.nl):
+            x[i] = self.stems[i](x[i])
+            reg_feat = self.reg_convs[i](x[i])
+            reg_output = self.reg_preds_lrtb[i](reg_feat)
+
+            cls_feat = self.cls_convs[i](x[i])
+            cls_output = self.cls_preds[i](cls_feat)
+            cls_output = torch.sigmoid(cls_output)
+
+            cls_sum = torch.clamp(cls_output.sum(1, keepdim=True), 0, 1)
+
+            output_for_rknn.append(reg_output)
+            output_for_rknn.append(cls_output)
+            output_for_rknn.append(cls_sum)
+        return output_for_rknn
+
     def forward(self, x):
+        if getattr(self, "export_rknn", False):
+            return self._rknn_opt_head(x)
+
         if self.training:
             cls_score_list = []
             reg_distri_list = []
diff --git a/yolov6/utils/checkpoint.py b/yolov6/utils/checkpoint.py
index c2f6239..6b67e45 100644
--- a/yolov6/utils/checkpoint.py
+++ b/yolov6/utils/checkpoint.py
@@ -10,7 +10,7 @@ from yolov6.utils.torch_utils import fuse_model
 
 def load_state_dict(weights, model, map_location=None):
     """Load weights from checkpoint file, only assign weights those layers' name and shape are match."""
-    ckpt = torch.load(weights, map_location=map_location)
+    ckpt = torch.load(weights, map_location=map_location, weights_only=False)
     state_dict = ckpt['model'].float().state_dict()
     model_state_dict = model.state_dict()
     state_dict = {k: v for k, v in state_dict.items() if k in model_state_dict and v.shape == model_state_dict[k].shape}
@@ -22,7 +22,7 @@ def load_state_dict(weights, model, map_location=None):
 def load_checkpoint(weights, map_location=None, inplace=True, fuse=True):
     """Load model from checkpoint file."""
     LOGGER.info("Loading checkpoint from {}".format(weights))
-    ckpt = torch.load(weights, map_location=map_location)  # load
+    ckpt = torch.load(weights, map_location=map_location, weights_only=False)  # load
     model = ckpt['ema' if ckpt.get('ema') else 'model'].float()
     if fuse:
         LOGGER.info("\nFusing model...")
@@ -49,7 +49,7 @@ def strip_optimizer(ckpt_dir, epoch):
         ckpt_path = osp.join(ckpt_dir, '{}_ckpt.pt'.format(s))
         if not osp.exists(ckpt_path):
             continue
-        ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'))
+        ckpt = torch.load(ckpt_path, map_location=torch.device('cpu'), weights_only=False)
         if ckpt.get('ema'):
             ckpt['model'] = ckpt['ema']  # replace model with ema
         for k in ['optimizer', 'ema', 'updates']:  # keys
